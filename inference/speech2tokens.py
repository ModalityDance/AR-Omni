#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
text_to_tokens_24khz.py
======================

Convert text -> speech -> discrete tokens at 24 kHz using:
- CosyVoice2 (TTS)
- WavTokenizer (audio codec tokenizer)

This module intentionally keeps the inference pipeline minimal for open-source use:
- No audio enhancement / filtering / VAD / TTA / smoothing
- Optional silence padding at the beginning/end (either in seconds or as token templates)

Notes
-----
- You must provide CosyVoice2 model files via `cosyvoice_model_dir`.
- WavTokenizer must be importable as a Python package, or provided via `wavtokenizer_root`
  (e.g., a git submodule path).
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
from pathlib import Path
from typing import List, Optional, Tuple, Union

import numpy as np
import soundfile as sf
import torch

logger = logging.getLogger(__name__)

__all__ = ["Text2TokenConverter"]


DEFAULT_PROMPT_TEXT = "Ask her to bring these things with her from the store."


def _add_to_syspath(path: Optional[Union[str, Path]]) -> None:
    if not path:
        return
    p = str(path)
    if p and p not in sys.path:
        sys.path.append(p)


def _read_wav_mono(path: Union[str, Path]) -> Tuple[np.ndarray, int]:
    wav, sr = sf.read(str(path))
    if wav.ndim > 1:
        wav = wav.mean(axis=1)
    return wav.astype(np.float32), int(sr)


def _flatten_codes_to_1d(codes: torch.Tensor) -> torch.Tensor:
    """
    Robustly flatten possible WavTokenizer code tensor shapes into a 1D sequence.

    Common shapes seen in the wild:
    - [B, T, nq]
    - [B, nq, T]
    - [nq, T]
    - [T]
    """
    c = codes
    if c.ndim >= 1 and c.shape[0] == 1:
        c = c.squeeze(0)

    if c.ndim == 3:
        # [T, nq] or [nq, T]
        c = c.squeeze(0)

    if c.ndim == 2:
        # Heuristic: treat as [nq, T] if first dim is small
        # (most codebooks nq is small like 1/2/4/8)
        if c.shape[0] > c.shape[1]:
            c = c.transpose(0, 1)
        c = c.reshape(-1)
    elif c.ndim == 1:
        pass
    else:
        raise ValueError(f"Unsupported codes ndim={c.ndim}, shape={tuple(c.shape)}")

    return c.to(torch.long)


class CosyVoice2Backend:
    """
    CosyVoice2 backend wrapper.

    You can optionally pass `cosyvoice_pythonpath` if CosyVoice is not installed
    as a package and you want to import it from a local checkout.
    """

    def __init__(
        self,
        model_dir: Union[str, Path],
        *,
        fp16: bool = False,
        cosyvoice_pythonpath: Optional[Union[str, Path]] = None,
    ) -> None:
        _add_to_syspath(cosyvoice_pythonpath)

        try:
            from cosyvoice.cli.cosyvoice import CosyVoice2
            from cosyvoice.utils.file_utils import load_wav
        except Exception as e:
            raise ImportError(
                "Failed to import CosyVoice2. Install CosyVoice as a package, or pass "
                "`cosyvoice_pythonpath` pointing to a local CosyVoice repo."
            ) from e

        self._load_wav = load_wav
        self._cv = CosyVoice2(
            str(model_dir),
            load_jit=False,
            load_trt=False,
            load_vllm=False,
            fp16=bool(fp16),
        )
        self.sample_rate = int(getattr(self._cv, "sample_rate", 24000))

    def tts(
        self,
        text: str,
        *,
        speaker_wav: Optional[Union[str, Path]] = None,
        prompt_text: Optional[str] = None,
    ) -> np.ndarray:
        prompt_text = prompt_text or DEFAULT_PROMPT_TEXT

        prompt_wav = None
        if speaker_wav:
            # CosyVoice load_wav expects 16k input for prompt
            prompt_wav = self._load_wav(str(speaker_wav), 16000)

        with torch.inference_mode():
            gen = self._cv.inference_zero_shot(
                text,
                prompt_text,
                prompt_wav,
                stream=False,
            )

        chunks: List[torch.Tensor] = []
        for item in gen:
            wav_t = item["tts_speech"]
            if not wav_t.dtype.is_floating_point:
                wav_t = wav_t.to(torch.float32)
            chunks.append(wav_t.contiguous().cpu())

        if not chunks:
            return np.zeros(0, dtype=np.float32)

        wav = torch.cat(chunks, dim=-1).clamp(-1, 1)
        wav = wav.squeeze(0).numpy().astype(np.float32)
        return wav


class Text2TokenConverter:
    """
    Convert text <-> tokens <-> waveform.

    Tokens are generated by encoding 24 kHz waveform with WavTokenizer.
    This class keeps only minimal padding features for open-source clarity:
    - silence head/tail insertion (seconds or token templates)
    """

    SENTINEL_BOS = 12821
    SENTINEL_EOS = 12820

    def __init__(
        self,
        *,
        cosyvoice_model_dir: Union[str, Path],
        device_id: int = 0,
        wavtokenizer_cfg_path: Union[str, Path],
        wavtokenizer_ckpt_path: Union[str, Path],
        wavtokenizer_root: Optional[Union[str, Path]] = None,
        cosyvoice_pythonpath: Optional[Union[str, Path]] = None,
        tok_shift: int = 8724,
        sample_rate: int = 24000,
        fp16_tts: bool = False,
        default_speaker_wav: Optional[Union[str, Path]] = "./inference/ref.wav",
    ) -> None:
        _add_to_syspath(wavtokenizer_root)

        self.device = (
            torch.device(f"cuda:{device_id}")
            if device_id >= 0 and torch.cuda.is_available()
            else torch.device("cpu")
        )
        self.sample_rate = int(sample_rate)
        self.tok_shift = int(tok_shift)
        self.default_speaker_wav = str(default_speaker_wav) if default_speaker_wav else None
        # Import WavTokenizer + resampler
        try:
            from encoder.utils import convert_audio
            from decoder.pretrained import WavTokenizer
        except Exception as e:
            raise ImportError(
                "Failed to import WavTokenizer. Ensure WavTokenizer is installed as a Python package, "
                "or pass `wavtokenizer_root` pointing to a local WavTokenizer repo/submodule."
            ) from e

        self._convert_audio = convert_audio
        self.wav_tok = WavTokenizer.from_pretrained0802(
            str(wavtokenizer_cfg_path),
            str(wavtokenizer_ckpt_path),
        ).to(self.device).eval()

        # TTS backend
        self._tts = CosyVoice2Backend(
            cosyvoice_model_dir,
            fp16=fp16_tts,
            cosyvoice_pythonpath=cosyvoice_pythonpath,
        )

    def _encode_wav_to_tokens_1d(self, wav_24k: torch.Tensor, bandwidth_id: int) -> List[int]:
        """
        wav_24k: [1, T] float32 on self.device
        Return: [BOS] + shifted_flat_codes + [EOS]
        """
        with torch.inference_mode():
            _, codes = self.wav_tok.encode_infer(
                wav_24k,
                bandwidth_id=torch.tensor([bandwidth_id], device=self.device),
            )
        flat = _flatten_codes_to_1d(codes)
        toks = (flat.cpu() + self.tok_shift).tolist()
        return [self.SENTINEL_BOS] + toks + [self.SENTINEL_EOS]

    def _silence_seconds_to_tokens(self, seconds: float, bandwidth_id: int) -> List[int]:
        if seconds <= 1e-9:
            return []
        dur = int(round(float(seconds) * self.sample_rate))
        if dur <= 0:
            return []
        wav_sil = torch.zeros(1, dur, dtype=torch.float32, device=self.device)
        toks = self._encode_wav_to_tokens_1d(wav_sil, bandwidth_id=bandwidth_id)
        # Drop BOS/EOS because caller will insert inside BOS/EOS
        return toks[1:-1]

    def _insert_silence_tokens(
        self,
        tokens: List[int],
        *,
        bandwidth_id: int,
        silence_head_sec: float = 0.2,
        silence_tail_sec: float = 0.2,
        silence_head_tokens: Optional[List[int]] = None,
        silence_tail_tokens: Optional[List[int]] = None,
    ) -> List[int]:
        if not tokens or tokens[0] != self.SENTINEL_BOS or tokens[-1] != self.SENTINEL_EOS:
            raise ValueError("Input tokens must start with BOS and end with EOS.")

        head = silence_head_tokens or self._silence_seconds_to_tokens(silence_head_sec, bandwidth_id)
        tail = silence_tail_tokens or self._silence_seconds_to_tokens(silence_tail_sec, bandwidth_id)

        if head:
            tokens = [tokens[0]] + list(head) + tokens[1:]
        if tail:
            tokens = tokens[:-1] + list(tail) + [tokens[-1]]
        return tokens

    def __call__(
        self,
        text: str,
        bandwidth_id: int = 0,
        return_wav: bool = False,
        **kwargs,
    ):
        """
        Text -> tokens (and optionally waveform).

        Supported kwargs:
        - speaker_wav: Optional[str|Path] prompt audio for zero-shot voice cloning
        - prompt_text: Optional[str] prompt text for CosyVoice2 inference
        - silence_head_sec / silence_tail_sec: float seconds of silence padding
        - silence_head_tokens / silence_tail_tokens: List[int] token templates to insert
        """
        speaker_wav = kwargs.get("speaker_wav", None)
        prompt_text = kwargs.get("prompt_text", None)

        silence_head_sec = float(kwargs.get("silence_head_sec", 0.0) or 0.0)
        silence_tail_sec = float(kwargs.get("silence_tail_sec", 0.0) or 0.0)
        silence_head_tokens = kwargs.get("silence_head_tokens", None)
        silence_tail_tokens = kwargs.get("silence_tail_tokens", None)

        if (speaker_wav is None or speaker_wav == "") and self.default_speaker_wav:
            speaker_wav = self.default_speaker_wav
        wav = self._tts.tts(
            text,
            speaker_wav=speaker_wav,
            prompt_text=prompt_text,
        )
        if wav.size == 0:
            raise RuntimeError("TTS produced empty audio.")

        wav_t = torch.from_numpy(wav).to(torch.float32).unsqueeze(0)  # [1, T]

        # Resample to 24 kHz if needed
        if self._tts.sample_rate != self.sample_rate:
            wav_t = self._convert_audio(wav_t, self._tts.sample_rate, self.sample_rate, 1).contiguous()

        wav_t = wav_t.to(self.device)

        tokens = self._encode_wav_to_tokens_1d(wav_t, bandwidth_id=bandwidth_id)
        tokens = self._insert_silence_tokens(
            tokens,
            bandwidth_id=bandwidth_id,
            silence_head_sec=silence_head_sec,
            silence_tail_sec=silence_tail_sec,
            silence_head_tokens=silence_head_tokens,
            silence_tail_tokens=silence_tail_tokens,
        )

        if return_wav:
            return tokens, wav_t.squeeze(0).detach().cpu()
        return tokens

    def tokens_to_wav(self, tokens: List[int], bandwidth_id: int = 0) -> torch.Tensor:
        """
        Tokens -> waveform using WavTokenizer decoder.
        """
        if tokens and tokens[0] == self.SENTINEL_BOS and tokens[-1] == self.SENTINEL_EOS:
            tokens = tokens[1:-1]

        codes = torch.tensor(tokens, dtype=torch.long, device=self.device) - self.tok_shift
        # WavTokenizer expects [B, nq, T] in many implementations; we keep the original behavior
        codes = codes.unsqueeze(0).unsqueeze(0)

        with torch.inference_mode():
            feats = self.wav_tok.codes_to_features(codes)
            wav = self.wav_tok.decode(
                feats,
                bandwidth_id=torch.tensor([bandwidth_id], device=self.device),
            )
        return wav.squeeze().detach().cpu()

    def wav_file_to_tokens(
        self,
        wav_path: Union[str, Path],
        bandwidth_id: int = 0,
        *,
        silence_head_sec: float = 0.0,
        silence_tail_sec: float = 0.0,
        silence_head_tokens: Optional[List[int]] = None,
        silence_tail_tokens: Optional[List[int]] = None,
    ) -> List[int]:
        """
        WAV file -> tokens.

        No preprocessing is applied except:
        - resampling to 24 kHz if needed
        - optional silence head/tail insertion
        """
        wav_np, sr = _read_wav_mono(wav_path)
        wav_t = torch.from_numpy(wav_np).to(torch.float32).unsqueeze(0)  # [1, T]

        if sr != self.sample_rate:
            wav_t = self._convert_audio(wav_t, sr, self.sample_rate, 1).contiguous()

        wav_t = wav_t.to(self.device)
        tokens = self._encode_wav_to_tokens_1d(wav_t, bandwidth_id=bandwidth_id)
        tokens = self._insert_silence_tokens(
            tokens,
            bandwidth_id=bandwidth_id,
            silence_head_sec=float(silence_head_sec),
            silence_tail_sec=float(silence_tail_sec),
            silence_head_tokens=silence_head_tokens,
            silence_tail_tokens=silence_tail_tokens,
        )
        return tokens


def _build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="Text <-> tokens at 24 kHz (CosyVoice2 + WavTokenizer)")
    p.add_argument("--device_id", type=int, default=0)

    p.add_argument("--cosyvoice_model_dir", type=str, required=True)
    p.add_argument("--cosyvoice_pythonpath", type=str, default=None)

    p.add_argument("--wavtokenizer_cfg_path", type=str, required=True)
    p.add_argument("--wavtokenizer_ckpt_path", type=str, required=True)
    p.add_argument("--wavtokenizer_root", type=str, default=None)

    p.add_argument("--bandwidth_id", type=int, default=0)
    p.add_argument("--tok_shift", type=int, default=8724)
    p.add_argument("--sample_rate", type=int, default=24000)
    p.add_argument("--fp16_tts", action="store_true")

    sub = p.add_subparsers(dest="mode", required=True)

    enc = sub.add_parser("encode", help="Text -> tokens")
    enc.add_argument("text", type=str)
    enc.add_argument("--speaker_wav", type=str, default=None)
    enc.add_argument("--prompt_text", type=str, default=None)
    enc.add_argument("--silence_head_sec", type=float, default=0.0)
    enc.add_argument("--silence_tail_sec", type=float, default=0.0)
    enc.add_argument("--out", type=str, default=None, help="Output JSON path (optional)")

    wavenc = sub.add_parser("encode-wav", help="WAV -> tokens")
    wavenc.add_argument("wav_path", type=str)
    wavenc.add_argument("--silence_head_sec", type=float, default=0.0)
    wavenc.add_argument("--silence_tail_sec", type=float, default=0.0)
    wavenc.add_argument("--out", type=str, default=None, help="Output JSON path (optional)")

    dec = sub.add_parser("decode", help="Tokens JSON -> wav")
    dec.add_argument("tokens_json", type=str)
    dec.add_argument("--wav_out", type=str, required=True)

    return p


def main() -> None:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    )

    args = _build_arg_parser().parse_args()

    conv = Text2TokenConverter(
        cosyvoice_model_dir=args.cosyvoice_model_dir,
        cosyvoice_pythonpath=args.cosyvoice_pythonpath,
        device_id=args.device_id,
        wavtokenizer_cfg_path=args.wavtokenizer_cfg_path,
        wavtokenizer_ckpt_path=args.wavtokenizer_ckpt_path,
        wavtokenizer_root=args.wavtokenizer_root,
        tok_shift=args.tok_shift,
        sample_rate=args.sample_rate,
        fp16_tts=args.fp16_tts,
    )

    if args.mode == "encode":
        tokens = conv(
            args.text,
            bandwidth_id=args.bandwidth_id,
            speaker_wav=args.speaker_wav,
            prompt_text=args.prompt_text,
            silence_head_sec=args.silence_head_sec,
            silence_tail_sec=args.silence_tail_sec,
        )
        s = json.dumps(tokens, ensure_ascii=False)
        if args.out:
            Path(args.out).parent.mkdir(parents=True, exist_ok=True)
            Path(args.out).write_text(s, encoding="utf-8")
            logger.info("Saved tokens to %s", args.out)
        else:
            print(s)

    elif args.mode == "encode-wav":
        tokens = conv.wav_file_to_tokens(
            args.wav_path,
            bandwidth_id=args.bandwidth_id,
            silence_head_sec=args.silence_head_sec,
            silence_tail_sec=args.silence_tail_sec,
        )
        s = json.dumps(tokens, ensure_ascii=False)
        if args.out:
            Path(args.out).parent.mkdir(parents=True, exist_ok=True)
            Path(args.out).write_text(s, encoding="utf-8")
            logger.info("Saved tokens to %s", args.out)
        else:
            print(s)

    elif args.mode == "decode":
        tokens = json.loads(Path(args.tokens_json).read_text(encoding="utf-8"))
        wav = conv.tokens_to_wav(tokens, bandwidth_id=args.bandwidth_id)
        Path(args.wav_out).parent.mkdir(parents=True, exist_ok=True)
        sf.write(args.wav_out, wav.numpy(), args.sample_rate)
        logger.info("Saved waveform to %s", args.wav_out)

    else:
        raise ValueError(f"Unknown mode: {args.mode}")


if __name__ == "__main__":
    main()
